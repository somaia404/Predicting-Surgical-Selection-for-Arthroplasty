from sklearn.model_selection import StratifiedKFold
from sklearn.utils.class_weight import compute_class_weight
import torch
import numpy as np
import pandas as pd
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, confusion_matrix, cohen_kappa_score, log_loss, average_precision_score, balanced_accuracy_score
from sklearn.calibration import calibration_curve
from sklearn.isotonic import IsotonicRegression
from tqdm import tqdm


# load CSV file including letter information and class
csv_file_path = "/content/hip_radiology_reports_finalised_SYNTH.csv"
data = pd.read_csv(csv_file_path)
# prepare data for training and testing, change labels to binary format
texts = data["Interpretation"].tolist()
label_map = {"No": 0, "Yes": 1}
labels = data["operated_on"].map(label_map).tolist()
print(len(labels))


# configure cross-validations
kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
sum_conf_matrix = np.zeros((2, 2), dtype=int)

# Reset lists to store metrics for each fold
accuracy_scores = []
f1_scores = []
recall_scores = []
precision_scores = []
roc_auc_scores = []
kappa_scores = []
confusion_matrices = []
train_losses_per_fold = []
val_losses_per_fold = []
calibration_fraction_of_positives = []
calibration_mean_predicted_value = []
# Add lists for new metrics
mcc_scores = []
balanced_accuracy_scores = []
log_loss_scores = []
auprc_scores = []


# Initialize model_path, tokenizer, and device (ensure these are defined)
model_path = "roberta-base"  # Using RoBERTa as determined in previous steps
tokenizer = AutoTokenizer.from_pretrained(model_path)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
max_length = 150
chosen_threshold = 0.3


# Use the combined original and augmented data (from random insertion)
texts_for_training = augmented_texts_combined_ri
labels_for_training = augmented_labels_combined_ri


# perform cross-validation. Split data then fine tune.
for fold, (train_index, test_index) in enumerate(kf.split(texts_for_training, labels_for_training)):
    print(f"Fold: {fold+1}")
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    model.to(device)
    if torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)

    train_texts = [texts_for_training[i] for i in train_index]
    train_labels = [labels_for_training[i] for i in train_index]
    test_texts = [texts_for_training[i] for i in test_index]
    test_labels = [labels_for_training[i] for i in test_index]

    # Calculate class weights for the current fold's training data
    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)
    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

    tokenized_inputs = tokenizer(train_texts, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt")
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6)  # Using a fixed lr for this task
    batch_size = 16  # Using a fixed batch size for this task
    epochs = 50  # Using a fixed number of epochs for this task
    num_samples = len(tokenized_inputs.input_ids)
    num_batches = (num_samples - 1) // batch_size + 1
    epoch_train_losses = []

    # Instantiate CrossEntropyLoss with class weights
    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)

    for epoch in range(epochs):
        optimizer.zero_grad()
        batch_losses = []  # Store losses for batches within an epoch
        for batch_idx in tqdm(range(num_batches), desc=f"Epoch {epoch+1}", unit  = "batch"):
            optimizer.zero_grad()
            batch_start = batch_idx * batch_size
            batch_end = min((batch_idx + 1) * batch_size, num_samples)
            batch_inputs = {k: v[batch_start:batch_end] for k, v in tokenized_inputs.items()}
            batch_labels = torch.tensor(train_labels[batch_start:batch_end])
            batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}
            batch_labels = batch_labels.to(device)
            outputs = model(**batch_inputs, labels=batch_labels)

            # Calculate loss using the weighted loss function
            loss = loss_fn(outputs.logits, batch_labels)

            if torch.cuda.device_count() > 1:
                loss = loss.mean()
            loss.backward()
            optimizer.step()
            batch_losses.append(loss.item())
        epoch_train_losses.append(np.mean(batch_losses))  # Average batch losses for epoch
        tqdm.write(f"Batch {batch_idx+1}/{num_batches}")  # Write after epoch progress bar

    train_losses_per_fold.append(epoch_train_losses)  # Store epoch losses per fold

    model.eval()
    test_outputs = []
    with torch.no_grad():
        for i in range(0, len(test_texts), batch_size):
            batch_texts = test_texts[i:i + batch_size]
            test_batch_labels = torch.tensor(test_labels[i:i + batch_size])
            tokenized_test_inputs = tokenizer(batch_texts, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt")
            batch_inputs2 = {k: v.to(device) for k, v in tokenized_test_inputs.items()}
            test_batch_labels = test_batch_labels.to(device)
            batch_outputs = model(**batch_inputs2)
            test_outputs.append(batch_outputs.logits)

    test_outputs = torch.cat(test_outputs, dim=0)
    probabilities = torch.softmax(test_outputs, dim=-1).cpu().detach().numpy()[:, 1]

    isotonic_regressor = IsotonicRegression(out_of_bounds='clip')
    calibrated_probs = isotonic_regressor.fit_transform(probabilities, test_labels)
    predictions = (calibrated_probs >= chosen_threshold).astype(int).tolist()

    fraction_of_positives, mean_predicted_value = calibration_curve(test_labels, calibrated_probs, n_bins=5)
    calibration_fraction_of_positives.append(fraction_of_positives)
    calibration_mean_predicted_value.append(mean_predicted_value)

    accuracy = accuracy_score(test_labels, predictions)
    f1 = f1_score(test_labels, predictions)
    recall = recall_score(test_labels, predictions)
    precision = precision_score(test_labels, predictions)
    roc_auc = roc_auc_score(test_labels, predictions)
    kappa = cohen_kappa_score(test_labels, predictions)
    conf_matrix = confusion_matrix(test_labels, predictions)

    # Calculate and store new metrics
    mcc = cohen_kappa_score(test_labels, predictions)  # MCC is same as Kappa for binary classification
    balanced_acc = balanced_accuracy_score(test_labels, predictions)
    logloss = log_loss(test_labels, probabilities)
    auprc = average_precision_score(test_labels, probabilities)

    accuracy_scores.append(accuracy)
    f1_scores.append(f1)
    recall_scores.append(recall)
    precision_scores.append(precision)
    roc_auc_scores.append(roc_auc)
    kappa_scores.append(kappa)
    confusion_matrices.append(conf_matrix)
    sum_conf_matrix += conf_matrix

    # Append new metrics
    mcc_scores.append(mcc)
    balanced_accuracy_scores.append(balanced_acc)
    log_loss_scores.append(logloss)
    auprc_scores.append(auprc)


    print(f"Fold {fold+1} Metrics:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1: {f1:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"AUROC: {roc_auc:.4f}")
    print(f"Kappa: {kappa:.4f}")
    # Print new metrics
    print(f"MCC: {mcc:.4f}")
    print(f"Balanced Accuracy: {balanced_acc:.4f}")
    print(f"Log-Loss: {logloss:.4f}")
    print(f"AUPRC: {auprc:.4f}")
    print("Confusion Matrix:")
    print(conf_matrix)
    print("Fold completed.")

# Calculate and print overall metrics
average_conf_matrix = sum_conf_matrix / len(confusion_matrices)
print("\nOverall Evaluation Metrics (New Model)")
print(f"Average accuracy: {sum(accuracy_scores) / len(accuracy_scores):.4f}")
print(f"Average F1: {sum(f1_scores) / len(f1_scores):.4f}")
print(f"Average recall: {sum(recall_scores) / len(recall_scores):.4f}")
print(f"Average precision: {sum(precision_scores) / len(precision_scores):.4f}")
print(f"Average AUROC: {sum(roc_auc_scores) / len(roc_auc_scores):.4f}")
print(f"Average Kappa: {sum(kappa_scores) / len(kappa_scores):.4f}")
# Print average of new metrics
print(f"Average MCC: {sum(mcc_scores) / len(mcc_scores):.4f}")
print(f"Average Balanced Accuracy: {sum(balanced_accuracy_scores) / len(balanced_accuracy_scores):.4f}")
print(f"Average Log-Loss: {sum(log_loss_scores) / len(log_loss_scores):.4f}")
print(f"Average AUPRC: {sum(auprc_scores) / len(auprc_scores):.4f}")
print(f"Average Confusion matrix:")
print(average_conf_matrix)
