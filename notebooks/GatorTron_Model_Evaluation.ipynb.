import pandas as pd
import torch
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, confusion_matrix, cohen_kappa_score, roc_curve
from sklearn.model_selection import StratifiedKFold
from sklearn.utils.class_weight import compute_class_weight
from sklearn.calibration import calibration_curve
from sklearn.isotonic import IsotonicRegression
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import matplotlib.pyplot as plt
from tqdm import tqdm
import os
import numpy as np
import seaborn as sns
from scipy.stats import sem, t

# Ensure CUDA is available
os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.cuda.empty_cache()
print(f"Using device: {device}")
print(f"CUDA is available: {torch.cuda.is_available()}")

# Load pretrained GatorTron model and tokenizer
model_path = "UFNLP/gatortron-base"
try:
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    model.to(device)
    if torch.cuda.device_count() > 1:
        print(f"Using {torch.cuda.device_count()} GPUs for data parallelism.")
        model = torch.nn.DataParallel(model)
except Exception as e:
    print(f"Error loading model: {e}")
    exit()

# Load CSV file
csv_file_path = "/content/hip_radiology_reports_finalised_SYNTH.csv"
try:
    data = pd.read_csv(csv_file_path)
except FileNotFoundError:
    print(f"Error: CSV file not found at {csv_file_path}")
    exit()

# Prepare data
texts = data["Interpretation"].tolist()
label_map = {"No": 0, "Yes": 1}
labels = data["operated_on"].map(label_map).tolist()
print(f"Number of samples: {len(labels)}")
print(f"Label distribution: {data['operated_on'].value_counts()}")

# Analyze text lengths
text_lengths = [len(text.split()) for text in data["Interpretation"]]
max_length = min(512, max(text_lengths)) # BERT max is 512
print(f"Maximum text length: {max(text_lengths)}")
print(f"Minimum text length: {min(text_lengths)}")
print(f"Average text length: {sum(text_lengths) / len(text_lengths):.2f}")
print(f"Using max_length for tokenization: {max_length}")

# Set up cross-validation
kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Lists to store results
accuracy_scores = []
f1_scores = []
recall_scores = []
precision_scores = []
roc_auc_scores = []
kappa_scores = []
confusion_matrices = []
train_losses_per_fold = []
all_test_labels = []
all_calibrated_probs = []

# Cross-validation loop
for fold, (train_index, test_index) in enumerate(kf.split(texts, labels)):
    print(f"==================== Fold: {fold+1} ====================")
    
    # Reload model for each fold to ensure fresh start
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    model.to(device)
    if torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)

    train_texts = [texts[i] for i in train_index]
    train_labels = [labels[i] for i in train_index]
    test_texts = [texts[i] for i in test_index]
    test_labels = [labels[i] for i in test_index]

    # Compute class weights
    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)
    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6)
    
    batch_size = 16
    epochs = 50
    
    train_losses = []
    
    # Training loop
    model.train()
    for epoch in range(epochs):
        epoch_losses = []
        # Create batches for training
        num_samples = len(train_texts)
        num_batches = (num_samples + batch_size - 1) // batch_size
        
        for batch_idx in tqdm(range(num_batches), desc=f"Epoch {epoch+1}/{epochs} (Fold {fold+1})", unit="batch"):
            batch_start = batch_idx * batch_size
            batch_end = min((batch_idx + 1) * batch_size, num_samples)
            
            batch_texts = train_texts[batch_start:batch_end]
            batch_labels = torch.tensor(train_labels[batch_start:batch_end]).to(device)
            
            optimizer.zero_grad()
            
            # Tokenize and run model
            tokenized_inputs = tokenizer(batch_texts, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt").to(device)
            outputs = model(**tokenized_inputs)
            
            # Calculate loss
            loss = loss_fn(outputs.logits, batch_labels)
            if isinstance(model, torch.nn.DataParallel):
                loss = loss.mean()
            
            # Backpropagation
            loss.backward()
            optimizer.step()
            
            epoch_losses.append(loss.item())
            
        train_losses.append(np.mean(epoch_losses))
    train_losses_per_fold.append(train_losses)
    
    # Evaluation loop
    model.eval()
    test_probabilities = []
    
    num_test_samples = len(test_texts)
    num_test_batches = (num_test_samples + batch_size - 1) // batch_size
    
    with torch.no_grad():
        for i in range(num_test_batches):
            batch_start = i * batch_size
            batch_end = min((i + 1) * batch_size, num_test_samples)
            
            batch_texts = test_texts[batch_start:batch_end]
            tokenized_test_inputs = tokenizer(batch_texts, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt").to(device)
            
            outputs = model(**tokenized_test_inputs)
            probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()[:, 1]
            test_probabilities.extend(probabilities)

    # Isotonic Regression for calibration
    isotonic_regressor = IsotonicRegression(out_of_bounds='clip')
    calibrated_probs = isotonic_regressor.fit_transform(test_probabilities, test_labels)
    
    # Choose a threshold and get predictions
    chosen_threshold = 0.3
    predictions = (calibrated_probs >= chosen_threshold).astype(int)
    
    # Store all test labels and predictions for overall metrics
    all_test_labels.extend(test_labels)
    all_calibrated_probs.extend(calibrated_probs)

    # Calculate metrics for the current fold
    accuracy = accuracy_score(test_labels, predictions)
    f1 = f1_score(test_labels, predictions)
    recall = recall_score(test_labels, predictions)
    precision = precision_score(test_labels, predictions)
    roc_auc = roc_auc_score(test_labels, predictions)
    kappa = cohen_kappa_score(test_labels, predictions)
    conf_matrix = confusion_matrix(test_labels, predictions)

    accuracy_scores.append(accuracy)
    f1_scores.append(f1)
    recall_scores.append(recall)
    precision_scores.append(precision)
    roc_auc_scores.append(roc_auc)
    kappa_scores.append(kappa)
    confusion_matrices.append(conf_matrix)

    print(f"\nFold {fold+1} Metrics:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1: {f1:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"AUROC: {roc_auc:.4f}")
    print(f"Kappa: {kappa:.4f}")
    print("Confusion Matrix:")
    print(conf_matrix)

# --- Overall Evaluation and Plotting ---
print("\n" + "="*50)
print("Overall Evaluation Metrics (Averaged over all folds)")

# Calculate average metrics
avg_accuracy = np.mean(accuracy_scores)
avg_f1 = np.mean(f1_scores)
avg_recall = np.mean(recall_scores)
avg_precision = np.mean(precision_scores)
avg_roc_auc = np.mean(roc_auc_scores)
avg_kappa = np.mean(kappa_scores)

print(f"Average Accuracy: {avg_accuracy:.4f}")
print(f"Average F1: {avg_f1:.4f}")
print(f"Average Recall: {avg_recall:.4f}")
print(f"Average Precision: {avg_precision:.4f}")
print(f"Average AUROC: {avg_roc_auc:.4f}")
print(f"Average Kappa: {avg_kappa:.4f}")

# Calculate and print standard error and confidence intervals
def calculate_standard_error(values, variable_name):
    mean_val = np.mean(values)
    standard_error = sem(values)
    dof = len(values) - 1
    confidence_interval = t.interval(0.95, dof, loc=mean_val, scale=standard_error)
    print(f"Standard Error of {variable_name}: {standard_error:.4f}")
    print(f"95% Confidence Interval of {variable_name}: ({confidence_interval[0]:.4f}, {confidence_interval[1]:.4f})")

calculate_standard_error(accuracy_scores, "Accuracy")
calculate_standard_error(f1_scores, "F1 Score")
calculate_standard_error(recall_scores, "Recall")
calculate_standard_error(precision_scores, "Precision")
calculate_standard_error(roc_auc_scores, "AUROC")
calculate_standard_error(kappa_scores, "Kappa")

# Average Confusion Matrix
avg_conf_matrix = np.mean(confusion_matrices, axis=0).astype(int)
print("\nAverage Confusion Matrix:")
print(avg_conf_matrix)

# Plotting
# Plot Training Loss
plt.figure(figsize=(10, 6))
for i, losses in enumerate(train_losses_per_fold):
    plt.plot(range(1, len(losses) + 1), losses, label=f"Fold {i+1}")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss Over Epochs (Per Fold)")
plt.legend()
plt.grid(True)
plt.show()

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(avg_conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not_Operation', 'Operation'],
            yticklabels=['Not_Operation', 'Operation'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Average Confusion Matrix')
plt.show()

# Plot ROC Curve
fpr, tpr, _ = roc_curve(all_test_labels, all_calibrated_probs)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUROC = {avg_roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Plot Calibration Curve
fraction_of_positives, mean_predicted_value = calibration_curve(all_test_labels, all_calibrated_probs, n_bins=10)
plt.figure(figsize=(8, 6))
plt.plot([0, 1], [0, 1], "k:", label="Perfect Calibration")
plt.plot(mean_predicted_value, fraction_of_positives, "s-", label="Calibrated Model")
plt.ylabel("Fraction of Positives")
plt.xlabel("Mean Predicted Value")
plt.title("Calibration Plot")
plt.legend()
plt.show()
